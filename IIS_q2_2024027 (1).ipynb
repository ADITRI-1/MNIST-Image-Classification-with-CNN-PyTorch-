{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bcc4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317e560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6c4d7",
   "metadata": {},
   "source": [
    "### Task 1 : Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38df6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e31c5474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:38<00:00, 259kB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 111kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:06<00:00, 254kB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.44MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load training and test datasets\n",
    "train_dataset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='.', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2092a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e45465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from the dataset\n",
    "def show_sample_images():\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(train_dataset.data[i], cmap='gray')\n",
    "        plt.title(f\"Label: {train_dataset.targets[i].item()}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mnist_samples.png')\n",
    "    plt.close()\n",
    "\n",
    "# Calculate and report class distribution\n",
    "def analyze_class_distribution():\n",
    "    # Count occurrences of each digit in the training set\n",
    "    train_labels = train_dataset.targets.numpy()\n",
    "    unique_labels, counts = np.unique(train_labels, return_counts=True)\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"Class Distribution in Training Set:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        print(f\"Digit {label}: {count} samples ({count/len(train_labels)*100:.2f}%)\")\n",
    "    \n",
    "    # Visualize the distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(unique_labels, counts)\n",
    "    plt.title('Class Distribution in MNIST Training Set')\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xticks(unique_labels)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return unique_labels, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27773f4f",
   "metadata": {},
   "source": [
    "### Task 2: CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b6e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Design and implement a CNN model\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Third convolutional layer for better feature extraction\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Prevent overfitting\n",
    "        self.fc2 = nn.Linear(512, 10)   # 10 output classes for digits 0-9\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        \n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf9f4f",
   "metadata": {},
   "source": [
    "### Task 3: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a2daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Split data into training and validation sets\n",
    "def train_model(model, train_loader, test_loader, epochs=15):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / len(test_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        test_losses.append(val_loss)\n",
    "        test_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), train_accuracies, label='Training')\n",
    "    plt.plot(range(1, epochs+1), test_accuracies, label='Validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), train_losses, label='Training')\n",
    "    plt.plot(range(1, epochs+1), test_losses, label='Validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.close()\n",
    "\n",
    "    torch.save(model.state_dict(), 'mnist_classifier.pth')\n",
    "    \n",
    "    return model, train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3a53d",
   "metadata": {},
   "source": [
    "### Task 4: Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9f07cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Identify most common misclassifications\n",
    "    errors = np.array(all_labels) != np.array(all_preds)\n",
    "    misclassified_indices = np.where(errors)[0]\n",
    "    \n",
    "    if len(misclassified_indices) > 0:\n",
    "        true_labels = np.array(all_labels)[misclassified_indices]\n",
    "        pred_labels = np.array(all_preds)[misclassified_indices]\n",
    "        \n",
    "        # Create a dictionary to count misclassifications\n",
    "        misclass_counts = {}\n",
    "        for true_label, pred_label in zip(true_labels, pred_labels):\n",
    "            pair = (true_label, pred_label)\n",
    "            misclass_counts[pair] = misclass_counts.get(pair, 0) + 1\n",
    "        \n",
    "        # Find the most common misclassifications\n",
    "        sorted_misclass = sorted(misclass_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\nMost Common Misclassifications:\")\n",
    "        for (true_label, pred_label), count in sorted_misclass[:5]:  # Show top 5\n",
    "            print(f\"True: {true_label}, Predicted: {pred_label}, Count: {count}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2216c9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing exploratory data analysis...\n",
      "Class Distribution in Training Set:\n",
      "Digit 0: 5923 samples (9.87%)\n",
      "Digit 1: 6742 samples (11.24%)\n",
      "Digit 2: 5958 samples (9.93%)\n",
      "Digit 3: 6131 samples (10.22%)\n",
      "Digit 4: 5842 samples (9.74%)\n",
      "Digit 5: 5421 samples (9.04%)\n",
      "Digit 6: 5918 samples (9.86%)\n",
      "Digit 7: 6265 samples (10.44%)\n",
      "Digit 8: 5851 samples (9.75%)\n",
      "Digit 9: 5949 samples (9.92%)\n",
      "\n",
      "Implementing CNN model...\n",
      "MNISTClassifier(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1152, out_features=512, bias=True)\n",
      "  (relu4): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Training the model...\n",
      "Epoch 1/15, Train Loss: 0.1769, Train Acc: 94.34%, Val Loss: 0.0437, Val Acc: 98.59%\n",
      "Epoch 2/15, Train Loss: 0.0514, Train Acc: 98.46%, Val Loss: 0.0342, Val Acc: 98.92%\n",
      "Epoch 3/15, Train Loss: 0.0383, Train Acc: 98.84%, Val Loss: 0.0285, Val Acc: 99.06%\n",
      "Epoch 4/15, Train Loss: 0.0306, Train Acc: 99.08%, Val Loss: 0.0267, Val Acc: 99.15%\n",
      "Epoch 5/15, Train Loss: 0.0255, Train Acc: 99.21%, Val Loss: 0.0190, Val Acc: 99.38%\n",
      "Epoch 6/15, Train Loss: 0.0199, Train Acc: 99.38%, Val Loss: 0.0252, Val Acc: 99.26%\n",
      "Epoch 7/15, Train Loss: 0.0180, Train Acc: 99.47%, Val Loss: 0.0207, Val Acc: 99.37%\n",
      "Epoch 8/15, Train Loss: 0.0173, Train Acc: 99.47%, Val Loss: 0.0341, Val Acc: 99.07%\n",
      "Epoch 9/15, Train Loss: 0.0148, Train Acc: 99.52%, Val Loss: 0.0256, Val Acc: 99.32%\n",
      "Epoch 10/15, Train Loss: 0.0125, Train Acc: 99.60%, Val Loss: 0.0321, Val Acc: 99.27%\n",
      "Epoch 11/15, Train Loss: 0.0126, Train Acc: 99.62%, Val Loss: 0.0301, Val Acc: 99.20%\n",
      "Epoch 12/15, Train Loss: 0.0128, Train Acc: 99.62%, Val Loss: 0.0269, Val Acc: 99.24%\n",
      "Epoch 13/15, Train Loss: 0.0102, Train Acc: 99.68%, Val Loss: 0.0291, Val Acc: 99.24%\n",
      "Epoch 14/15, Train Loss: 0.0073, Train Acc: 99.78%, Val Loss: 0.0309, Val Acc: 99.36%\n",
      "Epoch 15/15, Train Loss: 0.0102, Train Acc: 99.68%, Val Loss: 0.0322, Val Acc: 99.23%\n",
      "\n",
      "Evaluating the model...\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.9923\n",
      "Precision: 0.9923\n",
      "Recall: 0.9923\n",
      "F1 Score: 0.9923\n",
      "\n",
      "Most Common Misclassifications:\n",
      "True: 3, Predicted: 5, Count: 6\n",
      "True: 5, Predicted: 6, Count: 5\n",
      "True: 9, Predicted: 4, Count: 5\n",
      "True: 4, Predicted: 9, Count: 4\n",
      "True: 5, Predicted: 3, Count: 4\n",
      "\n",
      "Suggested Improvements:\n",
      "1. Data augmentation: Apply random rotations, shifts, and zoom to increase the diversity of training samples.\n",
      "2. Batch normalization: Add batch normalization layers after convolutions to stabilize training.\n",
      "3. Learning rate scheduling: Implement a learning rate schedule to reduce the learning rate over time.\n",
      "4. Architecture enhancements: Try residual connections or deeper architecture to improve feature extraction.\n",
      "5. Ensemble methods: Train multiple models and combine their predictions for better performance.\n"
     ]
    }
   ],
   "source": [
    " # Task 1: Data Loading and Exploration\n",
    "print(\"Performing exploratory data analysis...\")\n",
    "show_sample_images()\n",
    "unique_labels, counts = analyze_class_distribution()\n",
    "    \n",
    "# Task 2: Implement CNN model\n",
    "print(\"\\nImplementing CNN model...\")\n",
    "model = MNISTClassifier()\n",
    "print(model)\n",
    "    \n",
    "# Task 3: Train model\n",
    "print(\"\\nTraining the model...\")\n",
    "model, train_losses, train_accuracies, test_losses, test_accuracies = train_model(\n",
    "  model, train_loader, test_loader, epochs=15\n",
    ")\n",
    "   \n",
    "# Task 4: Evaluate model\n",
    "print(\"\\nEvaluating the model...\")\n",
    "accuracy, precision, recall, f1, cm = evaluate_model(model, test_loader)\n",
    "    \n",
    "# Suggest improvements\n",
    "print(\"\\nSuggested Improvements:\")\n",
    "print(\"1. Data augmentation: Apply random rotations, shifts, and zoom to increase the diversity of training samples.\")\n",
    "print(\"2. Batch normalization: Add batch normalization layers after convolutions to stabilize training.\")\n",
    "print(\"3. Learning rate scheduling: Implement a learning rate schedule to reduce the learning rate over time.\")\n",
    "print(\"4. Architecture enhancements: Try residual connections or deeper architecture to improve feature extraction.\")\n",
    "print(\"5. Ensemble methods: Train multiple models and combine their predictions for better performance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
